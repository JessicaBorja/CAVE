{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CAVE with AutoPyTorch\n",
    "\n",
    "AutoPyTorch provides a framework for automated neural-network-configuration. Currently it supports [BOHB](https://github.com/automl/HpBandSter) for hyperparameter search.\n",
    "CAVE integrates with AutoPyTorch, providing further insights and visualizations.\n",
    "This notebook provides an exemplary pipeline for using CAVE on / with AutoPyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate some AutoPyTorch-Output.\n",
    "You can use your own AutoPyTorch-routine here, we will use the openml-tasks, inspired by [AutoPyTorch's tutorial notebook](https://github.com/automl/Auto-PyTorch/blob/master/examples/basics/Auto-PyTorch%20Tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: This example adapts with the refactor of the APT project.\n",
    "Since logging is not yet finally implemented in the APT project, this example is not necessarily fully executable...\n",
    "However, feel free to open issues on errors you encounter in the [issue tracker](https://github.com/automl/CAVE/issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the old example output\n",
    "import os\n",
    "import logging\n",
    "import tempfile\n",
    "import shutil\n",
    "log_dir = \"logs/apt-cave-notebook/\"\n",
    "rerun_apt = False\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from autoPyTorch import AutoNetClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "import openml\n",
    "import json\n",
    "from ConfigSpace.read_and_write import json as pcs_json\n",
    "# Logging\n",
    "from autoPyTorch.components.metrics.additional_logs import *\n",
    "from autoPyTorch.pipeline.nodes import LogFunctionsSelector\n",
    "\n",
    "if rerun_apt:\n",
    "    # Remove old results\n",
    "    if os.path.exists(log_dir):\n",
    "        archive_path = shutil.make_archive(os.path.join(tempfile.mkdtemp(), '.OLD'), 'zip', log_dir)\n",
    "        shutil.rmtree(log_dir)\n",
    "        os.makedirs(log_dir)\n",
    "        shutil.move(archive_path, log_dir)\n",
    "    else:\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "    task = openml.tasks.get_task(task_id=31)\n",
    "\n",
    "    X, y = task.get_X_and_y()\n",
    "    ind_train, ind_test = task.get_train_test_split_indices()\n",
    "    X_train, Y_train = X[ind_train], y[ind_train]\n",
    "    X_test, Y_test = X[ind_test], y[ind_test]\n",
    "\n",
    "    autopytorch = AutoNetClassification(config_preset=\"medium_cs\",\n",
    "                                        result_logger_dir=log_dir,\n",
    "                                        log_every_n_datapoints=10,\n",
    "                                        use_tensorboard_logger=True,\n",
    "                                        additional_logs=[test_result.__name__,\n",
    "                                                         test_cross_entropy.__name__,\n",
    "                                                         test_balanced_accuracy.__name__],\n",
    "                                       )\n",
    "\n",
    "    # Get data from the openml task \"Supervised Classification on credit-g (https://www.openml.org/t/31)\"\n",
    "    task = openml.tasks.get_task(task_id=31)\n",
    "    X, y = task.get_X_and_y()\n",
    "    ind_train, ind_test = task.get_train_test_split_indices()\n",
    "    X_train, Y_train = X[ind_train], y[ind_train]\n",
    "    X_test, Y_test = X[ind_test], y[ind_test]\n",
    "    \n",
    "    \n",
    "    # Equip autopytorch with additional logs\n",
    "    gl = GradientLogger()\n",
    "    lw_gl = LayerWiseGradientLogger()\n",
    "    additional_logs = [gradient_max(gl), gradient_mean(gl), gradient_median(gl), gradient_std(gl),\n",
    "                       gradient_q10(gl), gradient_q25(gl), gradient_q75(gl), gradient_q90(gl),\n",
    "                       layer_wise_gradient_max(lw_gl), layer_wise_gradient_mean(lw_gl),\n",
    "                       layer_wise_gradient_median(lw_gl), layer_wise_gradient_std(lw_gl),\n",
    "                       layer_wise_gradient_q10(lw_gl), layer_wise_gradient_q25(lw_gl),\n",
    "                       layer_wise_gradient_q75(lw_gl), layer_wise_gradient_q90(lw_gl),\n",
    "                       gradient_norm()]\n",
    "\n",
    "    for additional_log in additional_logs:\n",
    "        autopytorch.pipeline[LogFunctionsSelector.get_name()].add_log_function(name=type(additional_log).__name__,\n",
    "                                                                               log_function=additional_log)\n",
    "\n",
    "        #sampling_space[\"additional_logs\"].append(type(additional_log).__name__)\n",
    "\n",
    "    autopytorch.pipeline[LogFunctionsSelector.get_name()].add_log_function(name=test_result.__name__, \n",
    "                                                                           log_function=test_result(autopytorch, X[ind_test], y[ind_test]))\n",
    "    autopytorch.pipeline[LogFunctionsSelector.get_name()].add_log_function(name=test_cross_entropy.__name__,\n",
    "                                                                           log_function=test_cross_entropy(autopytorch, X[ind_test], y[ind_test]))\n",
    "    autopytorch.pipeline[LogFunctionsSelector.get_name()].add_log_function(name=test_balanced_accuracy.__name__,\n",
    "                                                                           log_function=test_balanced_accuracy(autopytorch, X[ind_test], y[ind_test]))\n",
    "\n",
    "    # Fit to find an incumbent configuration with BOHB\n",
    "    results_fit = autopytorch.fit(X_train=X_train,\n",
    "                                  Y_train=Y_train,\n",
    "                                  validation_split=0.3,\n",
    "                                  max_runtime=750,\n",
    "                                  min_budget=10,\n",
    "                                  max_budget=50,\n",
    "                                  refit=True,\n",
    "                                 )\n",
    "    autopytorch.refit_all_incumbents(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: APT is supposed to automatically log the results to the output directory. Until then, do in manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if rerun_apt:\n",
    "    # Save fit results as json\n",
    "    with open(os.path.join(log_dir, \"results_fit.json\"), \"w\") as f:\n",
    "        json.dump(results_fit, f, indent=2)\n",
    "    \n",
    "    # Also necessary information (can be migrated either to CAVE or (preferably) to autopytorch)\n",
    "    with open(os.path.join(log_dir, 'configspace.json'), 'w') as f:\n",
    "        f.write(pcs_json.write(autopytorch.get_hyperparameter_search_space(X_train=X_train,\n",
    "                                                                       Y_train=Y_train)))\n",
    "    with open(os.path.join(log_dir, 'autonet_config.json'), 'w') as f:\n",
    "        json.dump(autopytorch.get_current_autonet_config(), f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, spin up CAVE pass along the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cave.cavefacade import CAVE\n",
    "\n",
    "cave_output_dir = \"cave_output\"\n",
    "\n",
    "cave = CAVE([log_dir],        # List of folders holding results\n",
    "            cave_output_dir,  # Output directory\n",
    "            ['.'],            # Target Algorithm Directory (only relevant for SMAC)\n",
    "            file_format=\"APT\",\n",
    "            verbose=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: should CAVE even get an autonet-instance? is all relevant information saved with info about the autonet-instance? would be nicer if there simply was some sort of scenario-file (which is partly/mostly covered by the results-dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cave.apt_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cave.compare_default_incumbent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other analyzers also run on the APT-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.apt_tensorboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
